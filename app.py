from flask import Flask, render_template, request, jsonify
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import mysql.connector
from sqlalchemy import create_engine, text
import re
import os

app = Flask(__name__)

# ---------------------------
# MySQL ì—°ê²° ë° DB/í…Œì´ë¸” ì„¤ì •
# ---------------------------
def create_connection_and_setup_db():
    # í™˜ê²½ ë³€ìˆ˜ ë””ë²„ê¹…
    print("ğŸ” í™˜ê²½ ë³€ìˆ˜ ë””ë²„ê¹…:")
    print(f"  DB_HOST: {os.environ.get('DB_HOST', 'NOT_SET')}")
    print(f"  DB_USER: {os.environ.get('DB_USER', 'NOT_SET')}")
    print(f"  DB_PASSWORD: {os.environ.get('DB_PASSWORD', 'NOT_SET')}")
    print(f"  DB_NAME: {os.environ.get('DB_NAME', 'NOT_SET')}")
    print(f"  HOST: {os.environ.get('HOST', 'NOT_SET')}")
    print(f"  PORT: {os.environ.get('PORT', 'NOT_SET')}")
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    db_host = os.environ.get('DB_HOST', 'localhost')
    db_user = os.environ.get('DB_USER', 'root')
    db_password = os.environ.get('DB_PASSWORD', 'q1w2e3r4')
    db_name = os.environ.get('DB_NAME', 'news')
    
    print(f"ğŸ”— ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹œë„: {db_user}@{db_host}/{db_name}")
    
    try:
        connection = mysql.connector.connect(
            host=db_host,
            user=db_user,
            password=db_password,
            database=db_name
        )
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ!")
        
        # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
        cursor = connection.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS news (
                id INT AUTO_INCREMENT PRIMARY KEY,
                title VARCHAR(500),
                link VARCHAR(1000),
                press VARCHAR(100),
                date DATE,
                time_desc VARCHAR(50)
            )
        """)
        connection.commit()
        print("âœ… ë‰´ìŠ¤ í…Œì´ë¸” í™•ì¸/ìƒì„± ì™„ë£Œ!")
        
        return connection
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        raise e

# ---------------------------
# ë‰´ìŠ¤ ì‚½ì… í•¨ìˆ˜
# ---------------------------
def insert_news(connection, title, link, press, date, time_desc):
    cursor = connection.cursor()
    cursor.execute(
        "INSERT INTO news (title, link, press, date, time_desc) VALUES (%s, %s, %s, %s, %s);",
        (title, link, press, date, time_desc)
    )
    connection.commit()

# ---------------------------
# ê¸°ì¡´ ë‰´ìŠ¤ ì‚­ì œ í•¨ìˆ˜ (ê²€ìƒ‰í•  ë•Œë§ˆë‹¤ ì‚­ì œ)
# ---------------------------
def delete_existing_news(connection):
    cursor = connection.cursor()
    cursor.execute("DELETE FROM news;")  # ê¸°ì¡´ ë‰´ìŠ¤ ëª¨ë‘ ì‚­ì œ
    connection.commit()

# ---------------------------
# í‚¤ì›Œë“œ í•„í„°ë§ í•¨ìˆ˜
# ---------------------------
def filter_news_by_keywords(news_list, include_keywords=None, exclude_keywords=None):
    """
    ë‰´ìŠ¤ ëª©ë¡ì„ í¬í•¨/ì œì™¸ í‚¤ì›Œë“œì— ë”°ë¼ í•„í„°ë§
    """
    if not include_keywords and not exclude_keywords:
        return news_list
    
    filtered_news = []
    
    for news in news_list:
        title = news['title'].lower()
        
        # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬ (í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ ì œì™¸)
        if exclude_keywords:
            exclude_keywords_list = [kw.strip().lower() for kw in exclude_keywords.split(',') if kw.strip()]
            if any(keyword in title for keyword in exclude_keywords_list):
                print(f"âŒ ì œì™¸ë¨: {news['title']} (ì œì™¸ í‚¤ì›Œë“œ: {[kw for kw in exclude_keywords_list if kw in title]})")
                continue
        
        # í¬í•¨ í‚¤ì›Œë“œ ì²´í¬ (ëª¨ë“  í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ì•¼ í•¨)
        if include_keywords:
            include_keywords_list = [kw.strip().lower() for kw in include_keywords.split(',') if kw.strip()]
            missing_keywords = [kw for kw in include_keywords_list if kw not in title]
            if missing_keywords:
                print(f"âŒ ì œì™¸ë¨: {news['title']} (ëˆ„ë½ëœ í‚¤ì›Œë“œ: {missing_keywords})")
                continue
            else:
                print(f"âœ… í¬í•¨ë¨: {news['title']} (ëª¨ë“  í‚¤ì›Œë“œ í¬í•¨)")
        
        filtered_news.append(news)
    
    return filtered_news

# ---------------------------
# Nate ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
# ---------------------------
def nate_news(keyword, page_count, connection):
    for page in range(1, page_count + 1):
        url = f'https://news.nate.com/search?q={keyword}&page={page}'
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.select('#search-option > div.search-result > ul > li')

        for article in articles:
            try:
                title = article.select_one('a > div.info > span > h2').text.replace("'", "''")
                link = article.select_one('a')['href']
                time_info = article.select_one('span.time').text.split()
                press = time_info[0]
                raw_time = time_info[1]
                time_desc = raw_time

                # ë‚ ì§œ ì²˜ë¦¬
                if 'ì „' in raw_time:
                    date = datetime.today().date()
                else:
                    date = datetime.strptime(raw_time, "%Y.%m.%d").date()

                insert_news(connection, title, link, press, date, time_desc)

            except Exception as e:
                print(f"âš ï¸ ë„¤ì´íŠ¸ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            continue

# ---------------------------
# Daum ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
# ---------------------------
def daum_news(keyword, page_count, connection):
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36"}
    for page in range(1, page_count + 1):
        url = f'https://search.daum.net/search?w=news&nil_search=btn&DA=PGD&enc=utf8&cluster=y&cluster_page=1&q={keyword}&p={page}'
        response = requests.get(url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.select('li[data-docid]')  # ë‰´ìŠ¤ ê°œë³„ í•­ëª© ì„ íƒ

        for article in articles:
            try:
                title_tag = article.select_one('.item-title strong.tit-g a')
                title = title_tag.text.strip().replace("'", "''") if title_tag else 'ì œëª© ì—†ìŒ'
                link = title_tag['href'] if title_tag else '#'

                press_tag = article.select_one('.area_tit a.item-writer strong.tit_item')
                if not press_tag:
                    press_tag = article.select_one('.area_tit a.item-writer span.txt_info')
                press = press_tag.text.strip() if press_tag else 'ì–¸ë¡ ì‚¬ ì—†ìŒ'

                time_tag = article.select_one('.item-contents span.txt_info')
                time_desc = time_tag.text.strip() if time_tag else 'ì‹œê°„ ì •ë³´ ì—†ìŒ'

                # ë‚ ì§œ ì²˜ë¦¬
                if 'ì „' in time_desc:
                    date = datetime.today().date()
                else:
                    try:
                        date = datetime.strptime(time_desc, "%Y.%m.%d.").date()
                    except:
                        date = datetime.today().date()

                insert_news(connection, title, link, press, date, time_desc)

            except Exception as e:
                print(f"âš ï¸ ë‹¤ìŒ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

# ---------------------------
# Flask ë¼ìš°íŠ¸ (ì›¹ í˜ì´ì§€)
# ---------------------------

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/crawl', methods=['POST'])
def crawl():
    keyword = request.form['keyword']
    page_count = int(request.form['page_count'])
    media = request.form['media']
    include_keywords = request.form.get('include_keywords', '').strip()
    exclude_keywords = request.form.get('exclude_keywords', '').strip()

    connection = create_connection_and_setup_db()

    # ê¸°ì¡´ ë‰´ìŠ¤ ì‚­ì œ
    delete_existing_news(connection)

    # ê²€ìƒ‰ì–´ êµ¬ì„±: ê¸°ë³¸ ê²€ìƒ‰ì–´ + í¬í•¨ í‚¤ì›Œë“œ
    search_keyword = keyword
    if include_keywords:
        include_list = [kw.strip() for kw in include_keywords.split(',') if kw.strip()]
        # ê¸°ë³¸ ê²€ìƒ‰ì–´ì™€ í¬í•¨ í‚¤ì›Œë“œë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” ê²€ìƒ‰ì–´ ìƒì„±
        search_keyword = f"{keyword} {' '.join(include_list)}"
        print(f"ğŸ” ì‹¤ì œ ê²€ìƒ‰ì–´: {search_keyword}")

    if media == 'ë„¤ì´íŠ¸':
        nate_news(search_keyword, page_count, connection)
    elif media == 'ë‹¤ìŒ':
        daum_news(search_keyword, page_count, connection)
    else:
        return jsonify({'error': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬í„¸ì…ë‹ˆë‹¤.'})

    # í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ì¶œë ¥ (pandas ëŒ€ì‹  SQLAlchemy ì‚¬ìš©)
    db_host = os.environ.get('DB_HOST', 'localhost')
    db_user = os.environ.get('DB_USER', 'root')
    db_password = os.environ.get('DB_PASSWORD', 'q1w2e3r4')
    db_name = os.environ.get('DB_NAME', 'news')
    
    engine = create_engine(f'mysql+mysqlconnector://{db_user}:{db_password}@{db_host}/{db_name}')
    
    # pandas ëŒ€ì‹  SQLAlchemyë¡œ ë°ì´í„° ì¡°íšŒ
    result = engine.execute(text('SELECT * FROM news ORDER BY date DESC'))
    news_list = []
    for row in result:
        news_list.append({
            'title': row.title,
            'link': row.link,
            'press': row.press,
            'date': row.date.isoformat() if row.date else None,
            'time_desc': row.time_desc
        })
    
    print(f"ğŸ“Š í¬ë¡¤ë§ëœ ì´ ë‰´ìŠ¤ ìˆ˜: {len(news_list)}")
    
    # í‚¤ì›Œë“œ í•„í„°ë§ ì ìš©
    filtered_news = filter_news_by_keywords(news_list, include_keywords, exclude_keywords)
    
    print(f"ğŸ” í•„í„°ë§ í›„ ë‰´ìŠ¤ ìˆ˜: {len(filtered_news)}")
    if include_keywords:
        print(f"âœ… í¬í•¨ í‚¤ì›Œë“œ: {include_keywords}")
    if exclude_keywords:
        print(f"âŒ ì œì™¸ í‚¤ì›Œë“œ: {exclude_keywords}")

    # JSON ì‘ë‹µìœ¼ë¡œ ë‰´ìŠ¤ ì „ë‹¬
    return jsonify(filtered_news)

if __name__ == '__main__':
    host = os.environ.get('HOST', '0.0.0.0')
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('DEBUG', 'False').lower() == 'true'
    
    print(f"ğŸŒ ì„œë²„ ì‹œì‘: http://{host}:{port}")
    app.run(host=host, port=port, debug=debug)
